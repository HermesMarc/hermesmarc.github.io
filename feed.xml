<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hermesmarc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hermesmarc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-26T09:30:01+00:00</updated><id>https://hermesmarc.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal Webpage which includes an overview on my work </subtitle><entry><title type="html">Not all Contradictions are created equal</title><link href="https://hermesmarc.github.io/blog/2023/contradictions/" rel="alternate" type="text/html" title="Not all Contradictions are created equal"/><published>2023-12-02T00:00:00+00:00</published><updated>2023-12-02T00:00:00+00:00</updated><id>https://hermesmarc.github.io/blog/2023/contradictions</id><content type="html" xml:base="https://hermesmarc.github.io/blog/2023/contradictions/"><![CDATA[<p>In logic and mathematics, we usually try to avoid them, but admittedly, sometimes they are also exactly what we are after: Contradictions. Not all contradictions are made equal, as we can identify at least two sources from which they can arise, and which are worth keeping in mind.</p> <p>To clarify the distinction, let‚Äôs consider the following very similar statements in arithmetic:</p> \[\begin{align} x &lt; y &amp;\land \neg \, x &lt; y \\ x &lt; y &amp;\land ~~ ~y \leq x \end{align}\] <p>Intuitively, both of them seem to simply express <em>‚Äù\(x\) is strictly smaller then \(y\) and \(y\) is less or equal to \(x\)‚Äú</em>, and intuition also tells us that this is a contradiction. While there is nothing to add to that conclusion, there <em>is</em> more to say about why both of them lead to a contradiction.</p> <p>The first statement \(x &lt; y \land \neg \, x &lt; y\) has the form \(A \land \neg A\), and is therefore a contradiction for purely logical reasons. By this I mean that: for the contradiction to arise, it doesn‚Äôt matter at all that we have a statement \(A := x &lt; y\) about numbers here.</p> <p>This is not the case for \(x &lt; y \land y \leq x\), since it does not have the form \(A \land \neg A\). We can however argue that \(y \leq x\) is equivalent to \(\neg \, x &lt; y\) , therefore allowing us to draw the same conclusion. Phew. Easy after all. But let‚Äôs consider yet another way to derive a contradiction here. For this, we dig a bit deeper and look at the actual definitions of \(&lt;\) and \(\leq\) in terms of successor, addition and equality.</p> \[\begin{align*} x &lt; y &amp;:= \exists k. ~ x + S k = y \\ y \leq x &amp;:= \exists k. ~ y + k = x \end{align*}\] <p>Given we start with \(x &lt; y\) and \(y \leq x\), we know that there are \(k\) and \(k'\) such that \(x + Sk = y\), and \(y + k' = x\). Combined this gives \(x + S k + k' = x\), and by cancelling \(x\) and both sides we get \(S(k + k') = 0\). Aha! This latter conclusion is of course fishy. One of the axioms of Peano arithmetic tells us that \(\forall x. \neg \, S x = 0\), which then brings us to the contradiction \(S (k + k') = 0 \land \neg \, S (k + k') = 0\).</p> <p>Let‚Äôs highlight some of the things that happened in this last proof:</p> <ul> <li>We arrive at a contradiction of the form \(A \land \neg A\), but this time the statement \(A\) is \(S (k + k') = 0\).</li> <li>To arrive at this conclusion we needed to make use of several results about number theory, including cancelation, and the axiom which states that zero has no predecessors.</li> </ul> <p>This is in contrast to the very first contradiction we saw above, where it did not matter that we were dealing with a statement involving numbers. In hindsight, we should realise at this point that when we used the equivalence of \(y \leq x\) and \(\neg \, x &lt; y\) as an easy way to the contradiction, we were forgetting that this equivalence also requires axioms in order to be shown. Oops.</p> <p>The above discussion points out two sources of contradictions:</p> <ol> <li>In can be for purely logical reasons</li> <li>It can be derived from an axiom which contains a negation in its statement.</li> </ol> <p>Needless to say; in the latter case we also make use of logical reasoning, but the point is that some contradictions cannot be reached without the right axiom, even in cases that appear obvious, like \(x &lt; y \land y \leq x\).</p>]]></content><author><name></name></author><category term="logic"/><category term="math"/><summary type="html"><![CDATA[In logic and mathematics, we usually try to avoid them, but admittedly, sometimes they are also exactly what we are after: Contradictions. Not all contradictions are made equal, as we can identify at least two sources from which they can arise, and which are worth keeping in mind.]]></summary></entry><entry><title type="html">Co-Leibniz Identity for Decidability</title><link href="https://hermesmarc.github.io/blog/2023/co-leibniz/" rel="alternate" type="text/html" title="Co-Leibniz Identity for Decidability"/><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://hermesmarc.github.io/blog/2023/co-leibniz</id><content type="html" xml:base="https://hermesmarc.github.io/blog/2023/co-leibniz/"><![CDATA[<p><a href="https://ncatlab.org/nlab/show/Heyting+algebra">Heyting algebras</a> are structures that can be used to give a semantics to intuitionistic propositional logic, and as it turns out, they can easily be dualised, yielding the aptly named <a href="https://ncatlab.org/nlab/show/co-Heyting+algebra"><em>co-Heyting algebras</em></a>.</p> <p>Co-Heyting algebras come equipped with three binary operations \((\land, \lor, \leftharpoondown)\), where \(\leftharpoondown\) is the dual of implication \(\to\) and usually called <em>subtraction</em> or <em>exclusion</em>.</p> <p>I will not go into too much detail on co-Heyting algebras here, but let me at least give you an intuition for why the name <em>subtraction</em> makes sense. In a Heyting algebra we have the following identity:</p> \[a \land b \leq c \iff a \leq (b \to c)\] <p>If you imagine the \(\leq\) to be another \(\to\), then the above reads \(a \land b \to c \iff a \to (b \to c)\) which should look like a familiar logical principle / currying of functions / adjunction of functors. The dualized identity in a co-Heyting algebra looks like this:</p> \[a \leq b \lor c \iff (a \leftharpoondown b) \leq c\] <p>And if you think of \(\lor\) as \(+\), then the above tells us that we can subtract \(b\) on both sides of the left equation without breaking the inequality.</p> <p>Let‚Äôs now move on to the main definition in co-Heyting algebras that I want to highlight here: the <a href="https://ncatlab.org/nlab/show/co-Heyting+boundary#definition"><em>boundary</em></a> of an element \(s\), which is defined by</p> \[\partial s := s \land \neg s.\] <p>The interesting thing about this definition of a boundary is that it satisfies the Leibniz rule familiar from calculus:</p> \[\partial (a \land b) = (\partial a \land b) \lor (a \land \partial b)\] <p>What surprised me was that <a href="https://ncatlab.org/nlab/show/Heyting+algebra#properties">nLab</a> does not mention any comparable definition for Heyting algebras, even though we have this strong duality between the two. And indeed, if we straight up dualise the definition, by flipping \(\land\) and \(\lor\) as well as the negation, we get an expression that looks oddly familiar</p> \[\delta s := s \lor \neg s\] <p>In constructive logic, \(\varphi \lor \neg \varphi\) is often referred to as the ‚Äúdecidability‚Äù of a statement \(\varphi\).</p> <p>Accordingly, we can indeed state and prove a dualised version of the Leibniz rule for this decidability operator:</p> \[\delta (a \lor b) = (\delta a \lor b) \land (a \lor \delta b)\] <p>This can be verified for any Heyting algebra, but below I give a quick verification of this fact by using the usual definition of decidability in the Coq proof assistant:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Definition iffT (X Y: Type) : Type := (X -&gt; Y) * (Y -&gt; X).
Notation "X &lt;=&gt; Y" := (iffT X Y) (at level 95).

Definition dec (P : Type) : Type := P + (P -&gt; False).

Lemma dec_sum A B :
  dec (A + B) &lt;=&gt; (dec A + B) * (A + dec B).
Proof.
  split; intros H; unfold dec.
  - destruct H as [|]; split; try tauto.
  - destruct H as [[[]|] [|[]]]; try tauto.
Qed.
</code></pre></div></div> <p>While I don‚Äôt have a good intuitive grasp on why the equivalence holds, I <em>can</em> give a good pictorial view on how to think about both \(\partial\) and \(\delta\).</p> <p>Consider a Venn-diagram showing overlapping sets sets \(A\) and \(B\). We can then think of the boundary \(\partial A\) as the line that we would use to outline the set \(A\), and likewise for the boundary of other sets. The Leibniz identity then simply reflects a way to compute the boundary \(\partial (A \cap B)\) based on the boundaries of \(A\) and \(B\). A similar visual explanation holds up for \(\delta A\); it consists of everything in the picture <em>except</em> the boundary \(\partial A\).</p> <p>Apart from the connection to decidability I showed above, I have not yet encountered the co-Leibniz identity elsewhere <em>‚Äúin the wild‚Äù</em>, and the same goes for people I have asked so far. So if you have, I would be interested to hear about it!</p>]]></content><author><name></name></author><category term="logic"/><category term="Heyting"/><category term="math"/><summary type="html"><![CDATA[Heyting algebras are structures that can be used to give a semantics to intuitionistic propositional logic, and as it turns out, they can easily be dualised, yielding the aptly named co-Heyting algebras.]]></summary></entry><entry><title type="html">üö¥üèª‚Äç‚ôÄÔ∏è Do finite Functions cycle üö¥üèø together? üö¥‚Äç‚ôÇÔ∏è</title><link href="https://hermesmarc.github.io/blog/2022/function-cycling/" rel="alternate" type="text/html" title="üö¥üèª‚Äç‚ôÄÔ∏è Do finite Functions cycle üö¥üèø together? üö¥‚Äç‚ôÇÔ∏è"/><published>2022-07-22T00:00:00+00:00</published><updated>2022-07-22T00:00:00+00:00</updated><id>https://hermesmarc.github.io/blog/2022/function-cycling</id><content type="html" xml:base="https://hermesmarc.github.io/blog/2022/function-cycling/"><![CDATA[<p>In this post I want to have a look at functions on finite sets and one of their inescapable passions: cycling.</p> <p>Given any set \(X\), element \(x \in X\) and function \(f : X \to X\) we can repeatably apply the function to \(x\) to get a infinite sequence of values \(f (x), f(f (x)), f( f(f (x))), \dots\) also written \(f^1 (x), f^2 (x), f^3 (x), \dots\) if we just add an exponent to \(f\) to indicate how often we applied it to \(x\).</p> <p>This sequence is not always infinitely interesting. In the extreme case, where \(X\) only has one element \(x_0\), the only possible function is \(x_0 \mapsto x_0\), giving us</p> \[f^1 (x_0), f^2 (x_0), f^3 (x_0), \dots \, = \,x_0, x_0, x_0, \dots\] <p>The result we get is quite repetitive, just continues on in the same fashion, never changes or adds something new, is therefore not very interesting, and is kind of just dragging along, just like this sentence. It gets a bit more interesting if we let \(X\) have a few more elements. Take \(X = \{1, 2, 3, 4\}\) for example, and the function defined by</p> \[f(1) := 3, ~f(2) := 2, ~f(3) := 4, ~f(4) := 1.\] <p>If you play around with some starting values (there aren‚Äôt that many) you will quickly see that repetition strikes again. Similarly to the very easy case of the one element set, here too our function seems to always end up in a cycle, but it can now contain more than one element. A more general principle seems to be at play, which brings us to our first puzzle:</p> <blockquote> <p><strong><em>Puzzle 1:</em></strong> Let \(f : X \to X\) be some function on a finite set \(X\). Show that there is an element \(a \in X\) to which \(f\) will always cycle back to.</p> </blockquote> <p>By <em>cycling back to \(a\)</em> we mean that there is some number of steps \(c \in \mathbb{N}\) such that \(f^c (a) = a\).</p> <p>So while puzzle 1 claims that functions on finite sets enjoy cycling on their own, what about cycling together?</p> <blockquote> <p><strong><em>Puzzle 2:</em></strong> Let \(X\) be a finite set. Show that there are numbers \(k &lt; c\) such that no matter the starting value \(x\), all functions \(f : X \to X\) will always cycle back to \(f^k(x)\) after \(c\) additional steps.</p> </blockquote> <p>Here we claim that after some time all functions will indeed cycle together, all of them returning to some personal anchor point after the same amount of time.</p> <hr/> <p>To be unambiguous with the puzzles, here are possible ways to state them in a fully formal way:</p> <blockquote> <p>Assuming \(X\) is finite, show:</p> <p>1) \(\forall (f : X \to X) \, \exists a \in X. ~\exists \, c. ~f^{c}(a) = a\).</p> <p>2) \(\exists \, k,c. ~~ k &lt; c ~\land~ \forall (f : X \to X) \, \forall x. ~ f^{c+k}(x) = f^k (x)\).</p> </blockquote>]]></content><author><name></name></author><category term="puzzle"/><summary type="html"><![CDATA[In this post I want to have a look at functions on finite sets and one of their inescapable passions: cycling.]]></summary></entry></feed>